{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Alzheimer's Classification with Swin Transformer & PSO Optimization\n",
                "\n",
                "This notebook provides a complete pipeline for training a Swin Transformer model on the Alzheimer's Dataset, including hyperparameter optimization using Particle Swarm Optimization (PSO) and an **Ablation Study** to verify component effectiveness.\n",
                "\n",
                "## 1. Setup and Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torchvision import datasets, transforms\n",
                "from torch.utils.data import DataLoader, Subset\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
                "import numpy as np\n",
                "import timm\n",
                "import random\n",
                "import matplotlib.pyplot as plt\n",
                "from tqdm.notebook import tqdm\n",
                "import cv2\n",
                "import platform\n",
                "import subprocess\n",
                "\n",
                "# Set seeds for reproducibility\n",
                "seed = 42\n",
                "torch.manual_seed(seed)\n",
                "np.random.seed(seed)\n",
                "random.seed(seed)\n",
                "\n",
                "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f\"Using device: {DEVICE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Dataset Management"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_transforms(phase='train', augment=True):\n",
                "    if phase == 'train':\n",
                "        if augment:\n",
                "            return transforms.Compose([\n",
                "                transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
                "                transforms.RandomHorizontalFlip(),\n",
                "                transforms.RandomVerticalFlip(p=0.05),\n",
                "                transforms.RandomRotation(15),\n",
                "                transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
                "                transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
                "                transforms.ToTensor(),\n",
                "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
                "            ])\n",
                "        else:\n",
                "            return transforms.Compose([\n",
                "                transforms.Resize((224, 224)),\n",
                "                transforms.RandomHorizontalFlip(),\n",
                "                transforms.ToTensor(),\n",
                "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
                "            ])\n",
                "    else:\n",
                "        return transforms.Compose([\n",
                "            transforms.Resize((224, 224)),\n",
                "            transforms.ToTensor(),\n",
                "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
                "        ])\n",
                "\n",
                "class AlzheimerDatasetManager:\n",
                "    def __init__(self, data_dir, batch_size=32, augment=True):\n",
                "        self.data_dir = data_dir\n",
                "        self.batch_size = batch_size\n",
                "        self.augment = augment\n",
                "        self.train_dir = os.path.join(data_dir, 'train')\n",
                "        self.test_dir = os.path.join(data_dir, 'test')\n",
                "        \n",
                "        # Load full datasets\n",
                "        self.full_train_dataset = datasets.ImageFolder(self.train_dir, transform=get_transforms('train', augment=augment))\n",
                "        self.full_test_dataset = datasets.ImageFolder(self.test_dir, transform=get_transforms('val'))\n",
                "        \n",
                "        self.classes = self.full_train_dataset.classes\n",
                "        print(f\"Classes found: {self.classes}\")\n",
                "\n",
                "    def get_kfold_loaders(self, n_splits=5):\n",
                "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
                "        targets = self.full_train_dataset.targets\n",
                "        \n",
                "        for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(targets)), targets)):\n",
                "            train_subset = Subset(self.full_train_dataset, train_idx)\n",
                "            val_subset = Subset(self.full_train_dataset, val_idx)\n",
                "            \n",
                "            train_loader = DataLoader(train_subset, batch_size=self.batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
                "            val_loader = DataLoader(val_subset, batch_size=self.batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
                "            \n",
                "            yield fold, train_loader, val_loader\n",
                "\n",
                "    def get_class_weights(self):\n",
                "        targets = np.array(self.full_train_dataset.targets)\n",
                "        class_counts = np.bincount(targets)\n",
                "        total_samples = len(targets)\n",
                "        weights = total_samples / (len(self.classes) * class_counts)\n",
                "        return weights\n",
                "\n",
                "    def get_test_loader(self):\n",
                "        return DataLoader(self.full_test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=2, pin_memory=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Model Builder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_model(num_classes=4, model_name='swin_tiny_patch4_window7_224', pretrained=True, dropout_rate=0.0):\n",
                "    print(f\"Building model: {model_name}\")\n",
                "    model = timm.create_model(model_name, pretrained=pretrained, num_classes=num_classes, drop_rate=dropout_rate)\n",
                "    return model\n",
                "\n",
                "def freeze_layers(model, freeze_backbone=False):\n",
                "    if freeze_backbone:\n",
                "        for param in model.parameters():\n",
                "            param.requires_grad = False\n",
                "        \n",
                "        # Unfreeze head\n",
                "        if hasattr(model, 'head'):\n",
                "            for param in model.head.parameters():\n",
                "                param.requires_grad = True\n",
                "    else:\n",
                "        for param in model.parameters():\n",
                "            param.requires_grad = True\n",
                "            \n",
                "    return model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. PSO Optimizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Particle:\n",
                "    def __init__(self, bounds):\n",
                "        self.position = {}\n",
                "        self.velocity = {}\n",
                "        self.best_position = {}\n",
                "        self.best_score = -1.0\n",
                "        \n",
                "        for key, (lower, upper) in bounds.items():\n",
                "            self.position[key] = random.uniform(lower, upper)\n",
                "            self.velocity[key] = random.uniform(-1, 1) * (upper - lower) * 0.1\n",
                "            self.best_position[key] = self.position[key]\n",
                "\n",
                "    def update_velocity(self, global_best_pos, w=0.5, c1=1.5, c2=1.5):\n",
                "        for key in self.position:\n",
                "            r1 = random.random()\n",
                "            r2 = random.random()\n",
                "            cognitive = c1 * r1 * (self.best_position[key] - self.position[key])\n",
                "            social = c2 * r2 * (global_best_pos[key] - self.position[key])\n",
                "            self.velocity[key] = w * self.velocity[key] + cognitive + social\n",
                "\n",
                "    def update_position(self, bounds):\n",
                "        for key in self.position:\n",
                "            self.position[key] += self.velocity[key]\n",
                "            lower, upper = bounds[key]\n",
                "            if self.position[key] < lower:\n",
                "                self.position[key] = lower\n",
                "                self.velocity[key] *= -1\n",
                "            elif self.position[key] > upper:\n",
                "                self.position[key] = upper\n",
                "                self.velocity[key] *= -1\n",
                "\n",
                "class PSOOptimizer:\n",
                "    def __init__(self, bounds, num_particles=5, iterations=3):\n",
                "        self.bounds = bounds\n",
                "        self.num_particles = num_particles\n",
                "        self.iterations = iterations\n",
                "        self.particles = [Particle(bounds) for _ in range(num_particles)]\n",
                "        self.global_best_position = None\n",
                "        self.global_best_score = -1.0\n",
                "\n",
                "    def optimize(self, fitness_function):\n",
                "        print(f\"Starting PSO Optimization with {self.num_particles} particles and {self.iterations} iterations.\")\n",
                "        self.global_best_position = self.particles[0].position.copy()\n",
                "        \n",
                "        for i in range(self.iterations):\n",
                "            print(f\"--- PSO Iteration {i+1}/{self.iterations} ---\")\n",
                "            for j, particle in enumerate(self.particles):\n",
                "                score = fitness_function(particle.position)\n",
                "                if score > particle.best_score:\n",
                "                    particle.best_score = score\n",
                "                    particle.best_position = particle.position.copy()\n",
                "                if score > self.global_best_score:\n",
                "                    self.global_best_score = score\n",
                "                    self.global_best_position = particle.position.copy()\n",
                "                    print(f\"New Global Best: {self.global_best_score:.4f} at {self.global_best_position}\")\n",
                "\n",
                "            for particle in self.particles:\n",
                "                particle.update_velocity(self.global_best_position)\n",
                "                particle.update_position(self.bounds)\n",
                "        return self.global_best_position, self.global_best_score"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Trainer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Trainer:\n",
                "    def __init__(self, model, device='cuda', class_weights=None):\n",
                "        self.model = model.to(device)\n",
                "        self.device = device\n",
                "        if class_weights is not None:\n",
                "            self.criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights).float().to(device))\n",
                "        else:\n",
                "            self.criterion = nn.CrossEntropyLoss()\n",
                "\n",
                "    def train_epoch(self, loader, optimizer):\n",
                "        self.model.train()\n",
                "        running_loss, all_preds, all_labels = 0.0, [], []\n",
                "        pbar = tqdm(loader, desc=\"Training\", leave=False)\n",
                "        for inputs, labels in pbar:\n",
                "            inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
                "            optimizer.zero_grad()\n",
                "            outputs = self.model(inputs)\n",
                "            loss = self.criterion(outputs, labels)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            running_loss += loss.item() * inputs.size(0)\n",
                "            _, preds = torch.max(outputs, 1)\n",
                "            all_preds.extend(preds.cpu().numpy())\n",
                "            all_labels.extend(labels.cpu().numpy())\n",
                "        return running_loss / len(loader.dataset), accuracy_score(all_labels, all_preds), f1_score(all_labels, all_preds, average='macro')\n",
                "\n",
                "    def evaluate(self, loader):\n",
                "        self.model.eval()\n",
                "        running_loss, all_preds, all_labels, all_probs = 0.0, [], [], []\n",
                "        with torch.no_grad():\n",
                "            for inputs, labels in loader:\n",
                "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
                "                outputs = self.model(inputs)\n",
                "                loss = self.criterion(outputs, labels)\n",
                "                running_loss += loss.item() * inputs.size(0)\n",
                "                _, preds = torch.max(outputs, 1)\n",
                "                probs = torch.softmax(outputs, dim=1)\n",
                "                all_preds.extend(preds.cpu().numpy())\n",
                "                all_labels.extend(labels.cpu().numpy())\n",
                "                all_probs.extend(probs.cpu().numpy())\n",
                "        return running_loss / len(loader.dataset), accuracy_score(all_labels, all_preds), f1_score(all_labels, all_preds, average='macro'), np.array(all_labels), np.array(all_preds), np.array(all_probs)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Visualization Utilities"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_confusion_matrix(cm, classes, filename='confusion_matrix.png'):\n",
                "    plt.figure(figsize=(10, 8))\n",
                "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
                "    plt.title('Confusion Matrix')\n",
                "    plt.colorbar()\n",
                "    tick_marks = np.arange(len(classes))\n",
                "    plt.xticks(tick_marks, classes, rotation=45)\n",
                "    plt.yticks(tick_marks, classes)\n",
                "    fmt = 'd'\n",
                "    thresh = cm.max() / 2.\n",
                "    for i, j in np.ndindex(cm.shape):\n",
                "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
                "    plt.tight_layout()\n",
                "    plt.ylabel('True label')\n",
                "    plt.xlabel('Predicted label')\n",
                "    plt.savefig(filename)\n",
                "    plt.show()\n",
                "\n",
                "def plot_ablation_results(results_dict, filename='ablation_results.png'):\n",
                "    variants = list(results_dict.keys())\n",
                "    scores = list(results_dict.values())\n",
                "    plt.figure(figsize=(10, 6))\n",
                "    bars = plt.bar(variants, scores, color=['skyblue', 'lightgreen', 'salmon'])\n",
                "    plt.title('Ablation Study: Macro F1-Score Comparison')\n",
                "    plt.ylabel('Macro F1-Score')\n",
                "    plt.ylim(0, 1.0)\n",
                "    for bar in bars:\n",
                "        yval = bar.get_height()\n",
                "        plt.text(bar.get_x() + bar.get_width()/2, yval + 0.01, f'{yval:.4f}', ha='center', va='bottom')\n",
                "    plt.tight_layout()\n",
                "    plt.savefig(filename)\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Configuration and Pipeline Execution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Tweak these metrics to check for best performance\n",
                "CONFIG = {\n",
                "    'data_dir': \"Alzheimer_s Dataset\",\n",
                "    'num_epochs': 25,       # Increase for more training\n",
                "    'num_folds': 5,         # Cross-validation folds\n",
                "    'pso_particles': 3,     # PSO swarm size\n",
                "    'pso_iterations': 2,    # PSO search depth\n",
                "    'batch_size': 32,\n",
                "    'learning_rate': 1e-4,\n",
                "    'weight_decay': 1e-4,\n",
                "    'dropout': 0.1\n",
                "}\n",
                "\n",
                "DATA_DIR = CONFIG['data_dir']\n",
                "RESULTS_DIR = \"results\"\n",
                "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
                "\n",
                "def run_training_variant(variant_name, dm, device, class_weights, params, epochs=10, folds=3):\n",
                "    print(f\"\\n>>> Running Variant: {variant_name}\")\n",
                "    fold_gen = dm.get_kfold_loaders(n_splits=folds)\n",
                "    f1_scores = []\n",
                "    for fold, train_loader, val_loader in fold_gen:\n",
                "        print(f\"  Fold {fold+1}/{folds}\")\n",
                "        model = build_model(num_classes=4, dropout_rate=params.get('dropout', 0.0)).to(device)\n",
                "        optimizer = torch.optim.AdamW(model.parameters(), lr=params.get('lr', 1e-4), weight_decay=params.get('weight_decay', 1e-4))\n",
                "        trainer = Trainer(model, device=device, class_weights=class_weights)\n",
                "        best_fold_f1 = 0.0\n",
                "        for _ in range(epochs):\n",
                "            trainer.train_epoch(train_loader, optimizer)\n",
                "            _, _, f1, _, _, _ = trainer.evaluate(val_loader)\n",
                "            best_fold_f1 = max(best_fold_f1, f1)\n",
                "        f1_scores.append(best_fold_f1)\n",
                "    return np.mean(f1_scores)\n",
                "\n",
                "if not os.path.exists(DATA_DIR):\n",
                "    print(f\"Warning: {DATA_DIR} not found.\")\n",
                "else:\n",
                "    dm = AlzheimerDatasetManager(DATA_DIR, batch_size=CONFIG['batch_size'])\n",
                "    class_weights = dm.get_class_weights()\n",
                "    \n",
                "    # --- Ablation Study ---\n",
                "    print(\"\\n--- Starting Ablation Study ---\")\n",
                "    ablation_results = {}\n",
                "    dm_baseline = AlzheimerDatasetManager(DATA_DIR, batch_size=CONFIG['batch_size'], augment=False)\n",
                "    ablation_results['Baseline'] = run_training_variant(\"Baseline\", dm_baseline, DEVICE, class_weights, {'lr': CONFIG['learning_rate'], 'weight_decay': CONFIG['weight_decay'], 'dropout': 0.0}, epochs=min(5, CONFIG['num_epochs']), folds=min(3, CONFIG['num_folds']))\n",
                "    ablation_results['Enhanced Aug'] = run_training_variant(\"Enhanced Aug\", dm, DEVICE, class_weights, {'lr': CONFIG['learning_rate'], 'weight_decay': CONFIG['weight_decay'], 'dropout': 0.0}, epochs=min(5, CONFIG['num_epochs']), folds=min(3, CONFIG['num_folds']))\n",
                "    \n",
                "    # --- PSO Optimization ---\n",
                "    def fitness_function(params):\n",
                "        lr, weight_decay, dropout = params.get('lr', 1e-4), params.get('weight_decay', 1e-4), params.get('dropout', 0.0)\n",
                "        model = build_model(num_classes=4, dropout_rate=dropout).to(DEVICE)\n",
                "        fold_gen = dm.get_kfold_loaders(n_splits=3)\n",
                "        _, train_loader, val_loader = next(fold_gen)\n",
                "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
                "        trainer = Trainer(model, device=DEVICE, class_weights=class_weights)\n",
                "        best_f1 = 0.0\n",
                "        for _ in range(2):\n",
                "            trainer.train_epoch(train_loader, optimizer)\n",
                "            _, _, f1, _, _, _ = trainer.evaluate(val_loader)\n",
                "            best_f1 = max(best_f1, f1)\n",
                "        return best_f1\n",
                "\n",
                "    bounds = {'lr': (1e-5, 1e-4), 'weight_decay': (1e-5, 1e-3), 'dropout': (0.0, 0.4)}\n",
                "    pso = PSOOptimizer(bounds, num_particles=CONFIG['pso_particles'], iterations=CONFIG['pso_iterations'])\n",
                "    best_params, best_f1_score = pso.optimize(fitness_function)\n",
                "    ablation_results['Proposed (PSO)'] = best_f1_score\n",
                "    plot_ablation_results(ablation_results, filename=os.path.join(RESULTS_DIR, 'ablation_study_chart.png'))\n",
                "\n",
                "    # --- Final CV Training --- (Using best_params)\n",
                "    fold_gen = dm.get_kfold_loaders(n_splits=CONFIG['num_folds'])\n",
                "    fold_results = {'f1': []}\n",
                "    best_model_state, best_overall_f1 = None, 0.0\n",
                "    for fold, train_loader, val_loader in tqdm(fold_gen, total=CONFIG['num_folds'], desc=\"K-Fold CV\"):\n",
                "        model = build_model(num_classes=4, dropout_rate=best_params['dropout']).to(DEVICE)\n",
                "        freeze_layers(model, freeze_backbone=True)\n",
                "        optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=best_params['lr'] * 10, weight_decay=best_params['weight_decay'])\n",
                "        trainer = Trainer(model, device=DEVICE, class_weights=class_weights)\n",
                "        for _ in range(5): trainer.train_epoch(train_loader, optimizer)\n",
                "        freeze_layers(model, freeze_backbone=False)\n",
                "        optimizer = torch.optim.AdamW(model.parameters(), lr=best_params['lr'], weight_decay=best_params['weight_decay'])\n",
                "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['num_epochs'])\n",
                "        best_fold_f1, patience, trigger_times = 0.0, 5, 0\n",
                "        for epoch in range(CONFIG['num_epochs']):\n",
                "            trainer.train_epoch(train_loader, optimizer)\n",
                "            _, _, f1, _, _, _ = trainer.evaluate(val_loader)\n",
                "            scheduler.step()\n",
                "            if f1 > best_fold_f1:\n",
                "                best_fold_f1, trigger_times = f1, 0\n",
                "                if best_fold_f1 > best_overall_f1:\n",
                "                    best_overall_f1, best_model_state = best_fold_f1, model.state_dict()\n",
                "                    torch.save(best_model_state, os.path.join(RESULTS_DIR, \"best_model.pth\"))\n",
                "            else:\n",
                "                trigger_times += 1\n",
                "                if trigger_times >= patience: break\n",
                "        fold_results['f1'].append(best_fold_f1)\n",
                "\n",
                "    # --- Final Test Evaluation ---\n",
                "    final_model = build_model(num_classes=4, dropout_rate=best_params['dropout'])\n",
                "    final_model.load_state_dict(best_model_state)\n",
                "    final_model = final_model.to(DEVICE)\n",
                "    test_loader = dm.get_test_loader()\n",
                "    trainer = Trainer(final_model, device=DEVICE)\n",
                "    _, acc, f1, labels, preds, _ = trainer.evaluate(test_loader)\n",
                "    report = f\"Ablation Results: {ablation_results}\\nPSO Params: {best_params}\\nFinal Test Accuracy: {acc:.4f}\\nTest F1: {f1:.4f}\\n\"\n",
                "    with open(os.path.join(RESULTS_DIR, 'report.txt'), 'w') as f: f.write(report)\n",
                "    plot_confusion_matrix(confusion_matrix(labels, preds), dm.classes, filename=os.path.join(RESULTS_DIR, 'final_confusion_matrix.png'))\n",
                "    print(report)"
            ]
        }\n",
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}